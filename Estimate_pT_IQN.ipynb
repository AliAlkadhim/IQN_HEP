{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1f538-b5de-4211-8a45-b2dcffb52552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard scientific python module\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as op\n",
    "\n",
    "# standard symbolic algebra module\n",
    "import sympy as sm\n",
    "sm.init_printing()\n",
    "\n",
    "# module to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# split data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# linearly transform a feature to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 18\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set usetex = False if LaTex is not \n",
    "# available on your system or if the \n",
    "# rendering is too slow\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "#seed = 128\n",
    "#rnd  = np.random.RandomState(seed)\n",
    "\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf71fd9-26e5-42f9-b81d-d4d5f469f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rawRecoDatapT</th>\n",
       "      <th>rawRecoDataeta</th>\n",
       "      <th>rawRecoDataphi</th>\n",
       "      <th>rawRecoDatam</th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0586</td>\n",
       "      <td>3.511970</td>\n",
       "      <td>1.503010</td>\n",
       "      <td>5.69919</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47.9465</td>\n",
       "      <td>0.776638</td>\n",
       "      <td>-1.251970</td>\n",
       "      <td>6.72517</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.5200</td>\n",
       "      <td>-1.084730</td>\n",
       "      <td>1.834230</td>\n",
       "      <td>4.06446</td>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23.2719</td>\n",
       "      <td>-2.822960</td>\n",
       "      <td>0.216718</td>\n",
       "      <td>3.50878</td>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.4644</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>1.306930</td>\n",
       "      <td>4.11101</td>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rawRecoDatapT  rawRecoDataeta  rawRecoDataphi  rawRecoDatam  \\\n",
       "0           0        29.0586        3.511970        1.503010       5.69919   \n",
       "1           1        47.9465        0.776638       -1.251970       6.72517   \n",
       "2           2        29.5200       -1.084730        1.834230       4.06446   \n",
       "3           3        23.2719       -2.822960        0.216718       3.50878   \n",
       "4           4        30.4644        2.985000        1.306930       4.11101   \n",
       "\n",
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e6ff8-1552-4ff9-aa0e-0cb31a6dacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:,5:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b26ef-0109-4c4b-a53c-d62d24f8dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  genDataphi  \\\n",
      "0         3.414790      1.47023   12.53740    45.4608    3.379820    1.470130   \n",
      "1         3.414790      1.47023   12.53740    56.2643    0.811412   -1.324120   \n",
      "2        -1.178620      1.84039    9.95503    34.6377   -1.131020    1.801820   \n",
      "3         2.133740     -2.86886    9.55921    27.4120   -2.842550    0.345529   \n",
      "4         2.964990      1.36464   10.69580    30.3263    3.040300    1.341270   \n",
      "...            ...          ...        ...        ...         ...         ...   \n",
      "99995    -3.226470     -1.29956    5.14477    25.5930   -3.147070   -1.366270   \n",
      "99996    -3.226470     -1.29956    5.14477    39.1548   -3.216660    2.488970   \n",
      "99997    -1.490050     -1.42238   11.15500    37.3361    2.942580    1.859210   \n",
      "99998    -0.654844     -1.26413    3.89300    27.9541   -0.672143   -1.214990   \n",
      "99999    -1.106860      1.31810    6.09530    30.6006   -1.130990    1.330040   \n",
      "\n",
      "       genDatam       tau  \n",
      "0      13.24440  0.536525  \n",
      "1      10.55060  0.130536  \n",
      "2       7.65844  0.500162  \n",
      "3       5.18675  0.490624  \n",
      "4       5.74890  0.417064  \n",
      "...         ...       ...  \n",
      "99995   5.17925  0.702844  \n",
      "99996   6.15084  0.280648  \n",
      "99997   8.82650  0.119586  \n",
      "99998   4.87261  0.554125  \n",
      "99999   3.94825  0.052025  \n",
      "\n",
      "[100000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "levels = ['genData', 'RecoData']\n",
    "kinematics=['pT','eta','phi','m']\n",
    "targets = kinematics#for reco level, but same names\n",
    "Networks = ['RecoNN', 'genNN']\n",
    "\n",
    "target = df['RecoDatapT'].to_numpy()\n",
    "data =  df.drop('RecoDatapT', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16158f-8207-4190-bb15-59784d3a5e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.3892, 40.3892, 29.3586, ..., 27.0034, 23.8815, 33.2208])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a3400-3651-4125-8256-e5ab5f48b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = 1\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, target, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c58998-0615-4815-903c-e721fccfcaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff906a-7669-482b-9588-0e101d0a1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target shape (80000,)\n",
      "input data shape (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "# train_targets = train_targets.reshape(-1,1)\n",
    "# test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "print('target shape', train_targets.shape)\n",
    "print('input data shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd1bfd-b236-45a2-aa29-810cb7966bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape =  (80000, 8) \n",
      "\n",
      "test_data shape =  (20000, 8) \n",
      "\n",
      "train_targets shape =  (80000, 1) \n",
      "\n",
      "test_targets shape =  (20000, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "sets= [train_data, test_data, train_targets, test_targets]\n",
    "set_names = ['train_data', 'test_data', 'train_targets', 'test_targets']\n",
    "# vnames = [name for name in globals() if globals()[name] is variable]\n",
    "\n",
    "def variable_string(variable):\n",
    "    return [k for k, v in locals().items() if v == variable][0]\n",
    "\n",
    "for var_name, var in zip(set_names, sets):\n",
    "    print(var_name \n",
    "          + ' shape = ', var.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666be72-f20e-4d3b-abc3-63d9bf24720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 100000\n",
      "\n",
      "Columns: ['rawRecoDatapT', 'rawRecoDataeta', 'rawRecoDataphi', 'rawRecoDatam', 'RecoDatapT', 'RecoDataeta', 'RecoDataphi', 'RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Fields: ['RecoDatapT', 'RecoDataeta', 'RecoDataphi', 'RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target: RecoDatapT\n",
      "\n",
      "Features: ['RecoDataeta', 'RecoDataphi', 'RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data    = pd.read_csv('Data.csv')\n",
    "print('number of entries:', len(data))\n",
    "\n",
    "columns = list(data.columns)[1:]\n",
    "print('\\nColumns:', columns)\n",
    "\n",
    "fields  = list(data.columns)[5:]\n",
    "print('\\nFields:', fields)\n",
    "\n",
    "target  = 'RecoDatapT'\n",
    "print('\\nTarget:', target )\n",
    "\n",
    "features= [x for x in fields]\n",
    "features.remove(target)\n",
    "\n",
    "print('\\nFeatures:', features)\n",
    "\n",
    "data    = data[fields]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9803af-7e71-4481-8531-a83da292e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:         75000\n",
      "validation set size:     5000\n",
      "test set size:          20000\n"
     ]
    }
   ],
   "source": [
    "# Fraction of the data assigned as test data\n",
    "fraction = 20/100\n",
    "# Split data into a part for training and a part for testing\n",
    "train_data, test_data = train_test_split(data, \n",
    "                                         test_size=fraction)\n",
    "\n",
    "# Split the training data into a part for training (fitting) and\n",
    "# a part for validating the training.\n",
    "fraction = 5/80\n",
    "train_data, valid_data = train_test_split(train_data, \n",
    "                                          test_size=fraction)\n",
    "\n",
    "# reset the indices in the dataframes and drop the old ones\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "test_data  = test_data.reset_index(drop=True)\n",
    "\n",
    "print('train set size:        %6d' % train_data.shape[0])\n",
    "print('validation set size:   %6d' % valid_data.shape[0])\n",
    "print('test set size:         %6d' % test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686d74e-90ed-4014-b1dc-12984e5450e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iqnutil.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iqnutil.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# return a batch of data for the next step in minimization\n",
    "def get_batch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"batch_size\" integers from \n",
    "    # the range [0, length-1] corresponding to the\n",
    "    # row indices.\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    return (batch_x, batch_t)\n",
    "\n",
    "# Note: there are several average loss functions available \n",
    "# in pytorch, but it's useful to know how to create your own.\n",
    "def average_quadratic_loss(f, t, x):\n",
    "    # f and t must be of the same shape\n",
    "    return  torch.mean((f - t)**2)\n",
    "\n",
    "def average_cross_entropy_loss(f, t, x):\n",
    "    # f and t must be of the same shape\n",
    "    loss = torch.where(t > 0.5, torch.log(f), torch.log(1 - f))\n",
    "    return -torch.mean(loss)\n",
    "\n",
    "def average_quantile_loss(f, t, x):\n",
    "    # f and t must be of the same shape\n",
    "    tau = x.T[-1] # last column is tau.\n",
    "    return torch.mean(torch.where(t >= f, \n",
    "                                  tau * (t - f), \n",
    "                                  (1 - tau)*(f - t)))\n",
    "\n",
    "# function to validate model during training.\n",
    "def validate(model, avloss, inputs, targets):\n",
    "    # make sure we set evaluation mode so that any training specific\n",
    "    # operations are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and t\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).float()\n",
    "        # remember to reshape!\n",
    "        o = model(x).reshape(t.shape)\n",
    "    return avloss(o, t, x)\n",
    "\n",
    "# A simple wrapper around a model to make using the latter more\n",
    "# convenient\n",
    "class ModelHandler:\n",
    "    def __init__(self, model, scalers):\n",
    "        self.model  = model\n",
    "        self.scaler_t, self.scaler_x = scalers\n",
    "        \n",
    "        self.scale  = self.scaler_t.scale_[0] # for output\n",
    "        self.mean   = self.scaler_t.mean_[0]  # for output\n",
    "        self.fields = self.scaler_x.feature_names_in_\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        \n",
    "        # scale input data\n",
    "        x  = np.array(self.scaler_x.transform(df[self.fields]))\n",
    "        x  = torch.Tensor(x)\n",
    "\n",
    "        # go to evaluation mode\n",
    "        self.model.eval()\n",
    "    \n",
    "        # compute,reshape to a 1d array, and convert to a numpy array\n",
    "        Y  = self.model(x).view(-1, ).detach().numpy()\n",
    "        \n",
    "        # rescale output\n",
    "        Y  = self.mean + self.scale * Y\n",
    "        \n",
    "        if len(Y) == 1:\n",
    "            return Y[0]\n",
    "        else:\n",
    "            return Y\n",
    "        \n",
    "    def show(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data)\n",
    "                print()\n",
    "        \n",
    "def train(model, optimizer, avloss, getbatch,\n",
    "          train_x, train_t, \n",
    "          valid_x, valid_t,\n",
    "          batch_size, \n",
    "          n_iterations, traces, \n",
    "          step=50):\n",
    "    \n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    n = len(valid_x)\n",
    "    \n",
    "    print('Iteration vs average loss')\n",
    "    print(\"%10s\\t%10s\\t%10s\" % \\\n",
    "          ('iteration', 'train-set', 'valid-set'))\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "\n",
    "        # set mode to training so that training specific \n",
    "        # operations such as dropout are enabled.\n",
    "        model.train()\n",
    "        \n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        \n",
    "        # convert the numpy arrays batch_x and batch_t to tensor \n",
    "        # types. The PyTorch tensor type is the magic that permits \n",
    "        # automatic differentiation with respect to parameters. \n",
    "        # However, since we do not need to take the derivatives\n",
    "        # with respect to x and t, we disable this feature\n",
    "        with torch.no_grad(): # no need to compute gradients \n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).float()      \n",
    "\n",
    "        # compute the output of the model for the batch of data x\n",
    "        # Note: outputs is \n",
    "        #   of shape (-1, 1), but the tensor targets, t, is\n",
    "        #   of shape (-1,)\n",
    "        # In order for the tensor operations with outputs and t\n",
    "        # to work correctly, it is necessary that they have the\n",
    "        # same shape. We can do this with the reshape method.\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "   \n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        empirical_risk.backward()   # compute gradients\n",
    "        \n",
    "        # finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer.step()            # move one step\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            \n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n]) \n",
    "            acc_v = validate(model, avloss, valid_x[:n], valid_t[:n])\n",
    "\n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % \\\n",
    "                      (xx[-1], acc_t, acc_v))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "                print(\"\\r%10d\\t%10.6f\\t%10.6f\" % \\\n",
    "                      (xx[-1], acc_t, acc_v), end='')\n",
    "                \n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "    print()      \n",
    "    return (xx, yy_t, yy_v)\n",
    "\n",
    "def plot_average_loss(traces, ftsize=18):\n",
    "    \n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    ax.plot(xx, yy_t, 'b', lw=2, label='Training')\n",
    "    ax.plot(xx, yy_v, 'r', lw=2, label='Validation')\n",
    "\n",
    "    ax.set_xlabel('Iterations', fontsize=ftsize)\n",
    "    ax.set_ylabel('average loss', fontsize=ftsize)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, which=\"both\", linestyle='-')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469b864-1b75-4b92-88e4-01f5c9b04a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iqnutil as ut\n",
    "importlib.reload(ut);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0db915-9089-4527-b618-394dad100534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iqn_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iqn_model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear( 8, 50),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Linear(50, 50),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Linear(50, 50),\n",
    "                      nn.ReLU(), \n",
    " \n",
    "                      nn.Linear(50, 50),\n",
    "                      nn.ReLU(), \n",
    " \n",
    "                      nn.Linear(50, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2da86524-a573-40e6-a180-1be898a5ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n",
      "Iteration vs average loss\n",
      " iteration\t train-set\t valid-set\n",
      "         0\t  0.312212\t  0.311025\n",
      "     58490\t  0.133441\t  0.138546"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_145130/1576879949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                   step=traces_step)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_average_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pulled_Github_Repositories/IQN_HEP/iqnutil.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, avloss, getbatch, train_x, train_t, valid_x, valid_t, batch_size, n_iterations, traces, step)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0macc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0macc_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pulled_Github_Repositories/IQN_HEP/iqnutil.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, avloss, inputs, targets)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# remember to reshape!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import iqn_model as iqn\n",
    "importlib.reload(iqn)\n",
    "model = iqn.model\n",
    "print(model)\n",
    "\n",
    "n_batch       = 50\n",
    "n_iterations  = 200000\n",
    "\n",
    "learning_rate = 2.e-4\n",
    "optimizer     = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=learning_rate) \n",
    "\n",
    "traces = ([], [], [])\n",
    "traces_step = 10\n",
    "\n",
    "traces = ut.train(model, optimizer, \n",
    "                  ut.average_quantile_loss,\n",
    "                  ut.get_batch,\n",
    "                  train_x, train_t, \n",
    "                  valid_x, valid_t,\n",
    "                  n_batch, \n",
    "                  n_iterations,\n",
    "                  traces,\n",
    "                  step=traces_step)\n",
    "\n",
    "ut.plot_average_loss(traces)\n",
    "\n",
    "# save model parameter dictionary\n",
    "torch.save(model.state_dict(), 'iqn_model.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6addd448-574f-44c9-b45b-7acd2ff4935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_t_x(df, target, source, scalers):\n",
    "#     # change from pandas dataframe format to a numpy array\n",
    "#     scaler_t, scaler_x = scalers\n",
    "#     t = np.array(scaler_t.transform(df[target].to_numpy().reshape(-1, 1)))\n",
    "#     x = np.array(scaler_x.transform(df[source]))\n",
    "#     t = t.reshape(-1,)\n",
    "#     return t, x\n",
    "\n",
    "# # create a scaler for target\n",
    "# scaler_t = StandardScaler()\n",
    "# scaler_t.fit(train_data[target].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# # create a scaler for inputs\n",
    "# scaler_x = StandardScaler()\n",
    "# scaler_x.fit(train_data[features])\n",
    "# # NB: undo scaling of tau, which is the last feature\n",
    "# scaler_x.mean_[-1] = 0\n",
    "# scaler_x.scale_[-1]= 1\n",
    "\n",
    "# scalers = [scaler_t, scaler_x]\n",
    "\n",
    "# train_t, train_x = split_t_x(train_data, target, features, scalers)\n",
    "# valid_t, valid_x = split_t_x(valid_data, target, features, scalers)\n",
    "# test_t,  test_x  = split_t_x(test_data,  target, features, scalers)\n",
    "\n",
    "# train_t.shape, train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3547fd-700a-4b2d-b1c9-7cf8c0297b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(0, slice(None, None, None))' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3747331/341441600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3747331/341441600.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcurrent_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcurrent_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m             \u001b[0mcasted_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(0, slice(None, None, None))' is an invalid key"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "print(train_dataset[0], train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f1b8c07a-a637-4f2b-8bc9-0c0184174e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=6, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f000e3c-e7c2-4044-92b3-8ab74b7574a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_145130/2220864302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    start=time.time()\n",
    "    while i<100:\n",
    "        data[\"x\"]\n",
    "    end =time.time()\n",
    "    s = end-start\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5929fbb5-d4a1-491a-b40c-5865b5479a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbatch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"batch_size\" integers from \n",
    "    # the range [0, length-1] corresponding to the\n",
    "    # row indices.\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    return (batch_x, batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6071393-9cc1-4735-8c2a-4e798d630b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e96587-d19a-4e3d-ae85-65f73f17a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape =  (75000, 9)\n"
     ]
    }
   ],
   "source": [
    "# n_examples, n_inputs = train_data.shape\n",
    "# n_outputs, n_hidden = 1, 16\n",
    "print('train_data.shape = ',train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "876d298d-2338-4843-beda-352a5d045eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (17): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=8, \n",
    "               hidden_size=16, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a5d5858-3b3e-4788-85eb-8146b6837564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %writefile training/RegressionEngine.py\n",
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def quadratic_loss(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "    @staticmethod\n",
    "    def average_quadratic_loss(targets, outputs):\n",
    "    # f and t must be of the same shape\n",
    "        return  torch.mean((outputs - targets)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_absolute_error(targets, outputs):\n",
    "    # f and t must be of the same shape\n",
    "        return  torch.mean(abs(outputs - targets))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def average_cross_entropy_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        loss = torch.where(targets > 0.5, torch.log(outputs), torch.log(1 - outputs))\n",
    "        # the above means loss = log outputs, if target>0.5, and log(1-output) otherwise\n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_quantile_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        tau = torch.rand(outputs.shape)\n",
    "        return torch.mean(torch.where(targets >= outputs, \n",
    "                                      tau * (targets - outputs), \n",
    "                                      (1 - tau)*(outputs - targets)))\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        with torch.no_grad(): # no need to compute gradients \n",
    "            # wrt. x and t\n",
    "            inputs = torch.from_numpy(batch_x).float()\n",
    "            targets = torch.from_numpy(batch_t).float() \n",
    "        self.optimizer.zero_grad()#only optimize weights for the current batch, otherwise it's meaningless!\n",
    "        # inputs = data[\"x\"]\n",
    "        # targets = data[\"y\"]\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.average_quantile_loss(targets, outputs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        # for data in data_loader:\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        with torch.no_grad(): # no need to compute gradients \n",
    "            # wrt. x and t\n",
    "            inputs = torch.from_numpy(batch_x).float()\n",
    "            targets = torch.from_numpy(batch_t).float() \n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.average_quantile_loss(targets, outputs)\n",
    "        final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0c8240b6-104c-4260-8da2-0a3c4393b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs, save_model=False):\n",
    "    train_losses, test_losses = [], []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    # EPOCHS=22\n",
    "    EPOCHS=epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.evaluate(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \"goodmodel.pth\")\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break\n",
    "    \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    train_losses=np.array(train_losses); test_losses=np.array(test_losses)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    epoch_list = np.arange(1, train_losses.shape[0]+1)\n",
    "    ax.plot(epoch_list, train_losses, label = 'Train')\n",
    "    ax.plot(epoch_list, test_losses, label='Test')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    return train_losses, test_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4f97be35-ab2c-4f0f-b82f-c32df5050768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 6.66314e-05, Test Loss: 0.000250517\n",
      "Epoch : 1         , Training Loss: 4.98445e-05, Test Loss: 0.000248616\n",
      "Epoch : 2         , Training Loss: 4.78598e-05, Test Loss: 0.000256384\n",
      "Epoch : 3         , Training Loss: 3.80107e-05, Test Loss: 0.000166971\n",
      "Epoch : 4         , Training Loss: 4.38641e-05, Test Loss: 0.000244381\n",
      "Epoch : 5         , Training Loss: 5.99354e-05, Test Loss: 0.000162521\n",
      "Epoch : 6         , Training Loss: 6.19582e-05, Test Loss: 0.000229702\n",
      "Epoch : 7         , Training Loss: 6.81725e-05, Test Loss: 0.000195681\n",
      "Epoch : 8         , Training Loss: 5.16224e-05, Test Loss: 0.000209519\n",
      "Epoch : 9         , Training Loss: 3.49498e-05, Test Loss: 0.000194027\n",
      "Epoch : 10        , Training Loss: 5.09155e-05, Test Loss: 0.000208201\n",
      "Epoch : 11        , Training Loss: 6.36909e-05, Test Loss: 0.00029882\n",
      "Epoch : 12        , Training Loss: 5.43787e-05, Test Loss: 0.000186571\n",
      "Epoch : 13        , Training Loss: 5.93053e-05, Test Loss: 0.000214789\n",
      "Epoch : 14        , Training Loss: 5.45759e-05, Test Loss: 0.000183252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "train_losses, test_losses=train(optimizer, \n",
    "      engine =RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 20,\n",
    "      epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f552c7-56b9-4059-8608-3459d65c5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_losses).shape, np.array(test_losses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "01c3cac7-20aa-46b8-ace9-031b77cb41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(1, train_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fce7cc65-2266-4098-b5fc-8950a02b4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]#y is Z values. I could add here my computed p-value for each theta,\n",
    "            #and make a dataframe col1:theta, col2: Z, col3, phat, col4: computedp-value\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])# number of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS.flatten(), LABELS.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "466955c7-7ca7-4272-8789-9c1a53b2f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [1. 1. 1. ... 1. 1. 1.]\n",
      "\n",
      "actual labels (targets Z):  [30.2673 21.5886 31.447  ... 21.2377 24.5727 21.1226]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01ec1c3d-6b82-4f0f-b5cc-b464d00c4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.8277e+04, 1.4950e+03, 1.8000e+02, 3.3000e+01, 6.0000e+00,\n",
       "        3.0000e+00, 3.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]),\n",
       " array([ 20.0008  ,  37.75662 ,  55.51244 ,  73.268265,  91.02408 ,\n",
       "        108.7799  , 126.53572 , 144.29155 , 162.04736 , 179.80319 ,\n",
       "        197.559   ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD+CAYAAAAtUeIJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALKklEQVR4nO3dP3JbV5rG4fdMeQFojZVqVPAOKO5gpKhTub0Dagfj7hW4pR2YswOX047s2YHEfAKzemL30Awm/ybApRtFQYA+4p9IPU8VA/CA4K1TV/gR99x7NaoqAPCx/uXYGwDA/SIcALQIBwAtwgFAi3AA0PLFsTdgn7788st6+vTpsTcD4F559+7dP6rq8YfGH3Q4nj59mrdv3x57MwDulTHG/6wbd6gKgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgJYHfeX4tp7++W9H+b1//+sfj/J7AT6GTxwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUDLF5ueMMaYJflTkhdV9fWtsZdJvknyXZLrJC+TXFfV+dJzzpJcTQ/nVfXm1mtsNQ7AYa0NxxjjJMk8izfu+QeedpLkXRbhOF9+Y79506+qH6fH8zHG91X1ahfjABze2nBU1UWSiykgH3rOV2te4lVVPVt67uUY43SH4wAc2N7WOKZDXKs+pVyNMZ5vO76zDQWgZeMaxybTm/gsi0NVJ0uHqm4Ocd12vTS2zTgAR7BtOC6zWAy/TJIxxtUY46eqepHk0Zqfm+1gfKVpXeQsSZ48ebLmJQC4i60OVVXVxU00bh4nOV23JrJvVXVeVadVdfr48eNjbQbAg7WPNY7LJOsWsGcbfn7bcQD26M7hmE6N/W3NU95m9eGmR0kudjAOwBFs+4njuxXfmyf5uaquszgDanZrfFZVW49vud0A3NHHhuO9v/yntY3r5e9NV5L/sLTu8TrTQvU0fpJk+U1/23EADmzTlePzLG4j8iLJyRjjdZJfbm4pUlXn01lMybT2sHxV9834FJRkccuQnY0DcHibrhy/TPJm+vrQc84/NHaIcQAOy91xAWgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoOWLTU8YY8yS/CnJi6r6esX4WZKr6eG8qt4cchyAw1objjHGSZJ5Fm/c8xXjZ0muqurH6fF8jPF9Vb06xDgAh7c2HFV1keRiCsgqr6rq2dLzL8cYpwccB+DA7rzGMR3Ceu9TSJKrMcbzfY/fbasB2NbGNY41bg5h3Xa9NLbPcQCOYJuzqh6tGZsdYHylMcbZGOPtGOPtr7/+uuYlALiLB3c6blWdV9VpVZ0+fvz42JsD8ODsIxyzI48DsEfbhONtVh9OepTk4gDjABzBncNRVddZnOE0uzU0q6qf9z1+1+0GYDsfG44PLVS/TnJ282C63uPnA44DcGCbrhyfJ3mZ5EWSkzHG6yS/VNV5sliIns5iejn9yHz5qu59jwNweJuuHL9M8mb6+tBzzje8xl7HATisB3c6LgD7JRwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC1fbPsCY4yXSb5J8l2S6yQvk1xX1fnSc86SXE0P51X15tZrbDUOwOFsHY7JSZJ3WYTjfPmN/eZNv6p+nB7PxxjfV9WrXYwDcFg7CUdVfbVm+FVVPVt67uUY43SH4wAc0F7XOMYYsyTzFUNXY4zn247vbEMB+Gg7+cQxvYnPsjhUdbJ0qGqef65NLLteGttmHIAD20U4LrNYDL9MkjHG1Rjjp6p6keTRmp+b7WD8PdOayFmSPHnyZM2PA3AXWx+qqqqLm2jcPE5yOsY42fa177g951V1WlWnjx8/PsYmADxo+1rjuEyybgF7tuHntx0HYE+2Csd0auxva57yNqsPNz1KcrGDcQAObBefOL5b8b15kp+r6jqLM6Bmt8ZnVbX1+LYbDkDfVuGY1jaul783XUn+w9K6x+tMi9XT+EmS5Tf9bccBOKCtz6qqqvPpTKZkWntYvqr7ZnwKSrK4ZcjOxgE4rF1dOX5+zHEADsfdcQFoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKDli2NvAO97+ue/HeX3/v2vfzzK7wXuF584AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAlntzr6oxxlmSq+nhvKreHHN7AD5X9yIcUzRSVT9Oj+djjO+r6tVxtwzg83MvwpHkVVU9u3lQVZdjjOfH3CCAz9UnH44xxizJyYqh6zHG86r6+cCb9GAd63buiVu6w33yyYcjyTzJ9YrvX2URFOF4APwfJHB/3IdwPMo/F8WXXSf519vfnNZDzqaH/zfG+O/9bdq98mWSfxx7Iz414/XWL2Fed8+c7kdnXv9t3eB9CEdLVZ0nOT/2dnxqxhhvq+r02Nvx0JjX3TOn+7HLeb0v13E8WvG9WZL/PfB2AHz27kM43mYRidseJbk47KYA8MmHo6quk1xOZ1ctmzmjqsXhu/0wr7tnTvdjZ/M6qmpXr7U304L3V1X17fT4JItrO1wACHBg9yIcye/xuMzisJVbjgAcyb0JB5uNMeZZRNUhPO4N++398+BOx/3MnST5z2k96DqLEwu+rarfTyJws8jNpkOhf6mqr1eMrZ0/8/tha+bVfruFm3v5Jbm5LdO309rw8vhO91nheGCq6g9jjNnyjnPDzSLXm97YvsniNO/5ivG182d+V9s0r4n99q7GGGfTtWu/P07yX5kisrd9tqp8PZCvJC83jL9b8b1fjr3dn9pXFn8Br5qrtfNnfu88r/bbu83nLMnZiu//luT5x8zdXef2kz8dl93YdLPIA2/OvbNp/szvfpjXteZJvl9xqcJlkvk+91nheGCmHeLm6z+WhjbdLJL1Ns2f+d2C/bavFmtAz+r9w3vzLNaJ9rbPWuN4WC6Sxf9XkiRjjMsxxk9V9SLNm0Xynk3zZ37vzn57R7V0AkGSjDFeJrmsqovpU8Ne9lnheEBu/uEtP54Wuz7rv8z4tNlvd2M69PSXJP++79/lUNXDd5nk5o6Ybha5nU3zZ353x37b9zrJ17cOXe1lnxWOB2L6C+1DV3Nexc0it7Vp/szvHdhvd2NaF3p969Pb3vZZ4Xg4rpKsOvf6NMlFuVnkVjbNn/m9M/vtlqZrMX5cjsb032pfZ0/7rHA8ECvOrLjZoX5Y2qFeZ3EM9Gbcf7272qqP78nm+TO/6703r/bb7UwL4G+XTiyY3TqVdi/7rHtVPTDTR9brTB9Ba/XtBdwscoXpnkkvs7jK+SSL21C/q/evzP3g/Jnf933kvNpvm6Z5/eUDw3+4ifI+9lnhAKDFoSoAWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWv4fOJ1d2CWNSN8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad4498-9888-4707-a7d5-504ac1c4dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phat_from_regressor(model, test_data):\n",
    "    X_torch = torch.from_numpy(X).float()\n",
    "    X_torch= Tensor(X_torch)\n",
    "    model.eval()\n",
    "    phat = model(X_torch)\n",
    "    phat = phat.squeeze()\n",
    "    phat=phat.detach().numpy().flatten()#detaches it from the computational history/prevent future computations from being tracked\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
