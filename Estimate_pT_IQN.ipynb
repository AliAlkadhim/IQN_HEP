{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bdc1f538-b5de-4211-8a45-b2dcffb52552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard scientific python module\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as op\n",
    "\n",
    "# standard symbolic algebra module\n",
    "#import sympy as sm\n",
    "#sm.init_printing()\n",
    "\n",
    "# module to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "# split data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# linearly transform a feature to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 18\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set usetex = False if LaTex is not \n",
    "# available on your system or if the \n",
    "# rendering is too slow\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "#seed = 128\n",
    "#rnd  = np.random.RandomState(seed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "aaf71fd9-26e5-42f9-b81d-d4d5f469f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rawRecoDatapT</th>\n",
       "      <th>rawRecoDataeta</th>\n",
       "      <th>rawRecoDataphi</th>\n",
       "      <th>rawRecoDatam</th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0586</td>\n",
       "      <td>3.511970</td>\n",
       "      <td>1.503010</td>\n",
       "      <td>5.69919</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47.9465</td>\n",
       "      <td>0.776638</td>\n",
       "      <td>-1.251970</td>\n",
       "      <td>6.72517</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.5200</td>\n",
       "      <td>-1.084730</td>\n",
       "      <td>1.834230</td>\n",
       "      <td>4.06446</td>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23.2719</td>\n",
       "      <td>-2.822960</td>\n",
       "      <td>0.216718</td>\n",
       "      <td>3.50878</td>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.4644</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>1.306930</td>\n",
       "      <td>4.11101</td>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rawRecoDatapT  rawRecoDataeta  rawRecoDataphi  rawRecoDatam  \\\n",
       "0           0        29.0586        3.511970        1.503010       5.69919   \n",
       "1           1        47.9465        0.776638       -1.251970       6.72517   \n",
       "2           2        29.5200       -1.084730        1.834230       4.06446   \n",
       "3           3        23.2719       -2.822960        0.216718       3.50878   \n",
       "4           4        30.4644        2.985000        1.306930       4.11101   \n",
       "\n",
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4a2e6ff8-1552-4ff9-aa0e-0cb31a6dacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:,5:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d28b26ef-0109-4c4b-a53c-d62d24f8dc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.41479   ,  1.47023   , 12.5374    , ...,  1.47013   ,\n",
       "        13.2444    ,  0.53652539],\n",
       "       [ 3.41479   ,  1.47023   , 12.5374    , ..., -1.32412   ,\n",
       "        10.5506    ,  0.13053623],\n",
       "       [-1.17862   ,  1.84039   ,  9.95503   , ...,  1.80182   ,\n",
       "         7.65844   ,  0.50016227],\n",
       "       ...,\n",
       "       [-1.49005   , -1.42238   , 11.155     , ...,  1.85921   ,\n",
       "         8.8265    ,  0.11958587],\n",
       "       [-0.654844  , -1.26413   ,  3.893     , ..., -1.21499   ,\n",
       "         4.87261   ,  0.5541255 ],\n",
       "       [-1.10686   ,  1.3181    ,  6.0953    , ...,  1.33004   ,\n",
       "         3.94825   ,  0.05202467]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = ['genData', 'RecoData']\n",
    "kinematics=['pT','eta','phi','m']\n",
    "targets = kinematics#for reco level, but same names\n",
    "Networks = ['RecoNN', 'genNN']\n",
    "\n",
    "target = df['RecoDatapT'].to_numpy()\n",
    "data =  df.drop('RecoDatapT', axis=1).to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e16158f-8207-4190-bb15-59784d3a5e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.3892, 40.3892, 29.3586, ..., 27.0034, 23.8815, 33.2208])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f3b3a95d-3bf3-4b7d-bf34-8c9a95ab2270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target shape (80000, 1)\n",
      "input data shape (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "# train_targets = train_targets.reshape(-1,1)\n",
    "# test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "print('target shape', train_targets.shape)\n",
    "print('input data shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "610a3400-3651-4125-8256-e5ab5f48b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = 1\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, target, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dafd1bfd-b236-45a2-aa29-810cb7966bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape =  (80000, 8) \n",
      "\n",
      "test_data shape =  (20000, 8) \n",
      "\n",
      "train_targets shape =  (80000, 1) \n",
      "\n",
      "test_targets shape =  (20000, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "sets= [train_data, test_data, train_targets, test_targets]\n",
    "set_names = ['train_data', 'test_data', 'train_targets', 'test_targets']\n",
    "# vnames = [name for name in globals() if globals()[name] is variable]\n",
    "\n",
    "def variable_string(variable):\n",
    "    return [k for k, v in locals().items() if v == variable][0]\n",
    "\n",
    "for var_name, var in zip(set_names, sets):\n",
    "    print(var_name \n",
    "          + ' shape = ', var.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "68142b60-eb97-4866-94f6-3d90a25782ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()#this is always recommended for logistic regression\n",
    "# train_data= sc.fit_transform(train_data)\n",
    "# test_data = sc.transform(test_data)\n",
    "# train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1e3547fd-700a-4b2d-b1c9-7cf8c0297b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([ 3.9172e+00, -1.6980e+00,  5.8360e+00,  3.2731e+01, -3.0877e-02,\n",
      "         1.7757e+00,  6.8790e+00,  4.9855e-01]), 'y': tensor([30.9906])} <__main__.CustomDataset object at 0x7fdb7143bb90>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "print(train_dataset[0], train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f1b8c07a-a637-4f2b-8bc9-0c0184174e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=2, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d6071393-9cc1-4735-8c2a-4e798d630b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "89e96587-d19a-4e3d-ae85-65f73f17a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape =  (80000, 8)\n"
     ]
    }
   ],
   "source": [
    "# n_examples, n_inputs = train_data.shape\n",
    "# n_outputs, n_hidden = 1, 16\n",
    "print('train_data.shape = ',train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "876d298d-2338-4843-beda-352a5d045eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (13): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (17): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (21): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): Dropout(p=0.3, inplace=False)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (25): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): Dropout(p=0.3, inplace=False)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (29): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): Dropout(p=0.3, inplace=False)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (33): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=8, \n",
    "               hidden_size=16, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8a5d5858-3b3e-4788-85eb-8146b6837564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %writefile training/RegressionEngine.py\n",
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def quadratic_loss(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "    @staticmethod\n",
    "    def average_quadratic_loss(targets, outputs):\n",
    "    # f and t must be of the same shape\n",
    "        return  torch.mean((outputs - targets)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_cross_entropy_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        loss = torch.where(targets > 0.5, torch.log(outputs), torch.log(1 - outputs))\n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_quantile_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        tau = torch.rand(outputs.shape)\n",
    "        return torch.mean(torch.where(targets >= outputs, \n",
    "                                      tau * (targets - outputs), \n",
    "                                      (1 - tau)*(outputs - targets)))\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()#only optimize weights for the current batch, otherwise it's meaningless!\n",
    "            inputs = data[\"x\"]\n",
    "            targets = data[\"y\"]\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.average_quadratic_loss(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.average_quadratic_loss(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "            return outputs.flatten()\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0c8240b6-104c-4260-8da2-0a3c4393b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs):\n",
    "    train_losses, test_losses = [], []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    # EPOCHS=22\n",
    "    EPOCHS=epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.train(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break\n",
    "    \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    train_losses=np.array(train_losses); test_losses=np.array(test_losses)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    epoch_list = np.arange(1, train_losses.shape[0]+1)\n",
    "    ax.plot(epoch_list, train_losses, label = 'Train')\n",
    "    ax.plot(epoch_list, test_losses, label='Test')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    return train_losses, test_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4f97be35-ab2c-4f0f-b82f-c32df5050768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 0.042679  , Test Loss: 0.153787  \n",
      "Epoch : 1         , Training Loss: 0.0372745 , Test Loss: 0.153704  \n",
      "Epoch : 2         , Training Loss: 0.0429318 , Test Loss: 0.153487  \n",
      "Epoch : 3         , Training Loss: 0.0493629 , Test Loss: 0.153351  \n",
      "Epoch : 4         , Training Loss: 0.0433362 , Test Loss: 0.153493  \n",
      "Epoch : 5         , Training Loss: 0.0817129 , Test Loss: 0.153372  \n",
      "Epoch : 6         , Training Loss: 0.0290774 , Test Loss: 0.153367  \n",
      "Epoch : 7         , Training Loss: 0.0309108 , Test Loss: 0.153412  \n",
      "Epoch : 8         , Training Loss: 0.0539307 , Test Loss: 0.153808  \n",
      "Epoch : 9         , Training Loss: 0.0592346 , Test Loss: 0.15321   \n",
      "Epoch : 10        , Training Loss: 0.038731  , Test Loss: 0.153532  \n",
      "Epoch : 11        , Training Loss: 0.038944  , Test Loss: 0.153238  \n",
      "Epoch : 12        , Training Loss: 0.0397257 , Test Loss: 0.153465  \n",
      "Epoch : 13        , Training Loss: 0.050866  , Test Loss: 0.153134  \n",
      "Epoch : 14        , Training Loss: 0.050332  , Test Loss: 0.153603  \n",
      "Epoch : 15        , Training Loss: 0.0789458 , Test Loss: 0.153524  \n",
      "Epoch : 16        , Training Loss: 0.034314  , Test Loss: 0.15327   \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_losses, test_losses=train(optimizer, \n",
    "      engine =RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 100,\n",
    "      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "72f552c7-56b9-4059-8608-3459d65c5144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16,), (16,))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_losses).shape, np.array(test_losses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "01c3cac7-20aa-46b8-ace9-031b77cb41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(1, train_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fce7cc65-2266-4098-b5fc-8950a02b4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]#y is Z values. I could add here my computed p-value for each theta,\n",
    "            #and make a dataframe col1:theta, col2: Z, col3, phat, col4: computedp-value\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])# number of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS.flatten(), LABELS.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "466955c7-7ca7-4272-8789-9c1a53b2f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [0.89188945 0.93900347 0.8798901  ... 0.91908    0.92923063 0.9897389 ]\n",
      "\n",
      "actual labels (targets Z):  [28.2491 28.3123 28.2231 ... 20.7069 24.4039 29.9623]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01ec1c3d-6b82-4f0f-b5cc-b464d00c4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.8277e+04, 1.4950e+03, 1.8000e+02, 3.3000e+01, 6.0000e+00,\n",
       "        3.0000e+00, 3.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]),\n",
       " array([ 20.0008  ,  37.75662 ,  55.51244 ,  73.268265,  91.02408 ,\n",
       "        108.7799  , 126.53572 , 144.29155 , 162.04736 , 179.80319 ,\n",
       "        197.559   ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD+CAYAAAAtUeIJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALKklEQVR4nO3dP3JbV5rG4fdMeQFojZVqVPAOKO5gpKhTub0Dagfj7hW4pR2YswOX047s2YHEfAKzemL30Awm/ybApRtFQYA+4p9IPU8VA/CA4K1TV/gR99x7NaoqAPCx/uXYGwDA/SIcALQIBwAtwgFAi3AA0PLFsTdgn7788st6+vTpsTcD4F559+7dP6rq8YfGH3Q4nj59mrdv3x57MwDulTHG/6wbd6gKgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgJYHfeX4tp7++W9H+b1//+sfj/J7AT6GTxwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUDLF5ueMMaYJflTkhdV9fWtsZdJvknyXZLrJC+TXFfV+dJzzpJcTQ/nVfXm1mtsNQ7AYa0NxxjjJMk8izfu+QeedpLkXRbhOF9+Y79506+qH6fH8zHG91X1ahfjABze2nBU1UWSiykgH3rOV2te4lVVPVt67uUY43SH4wAc2N7WOKZDXKs+pVyNMZ5vO76zDQWgZeMaxybTm/gsi0NVJ0uHqm4Ocd12vTS2zTgAR7BtOC6zWAy/TJIxxtUY46eqepHk0Zqfm+1gfKVpXeQsSZ48ebLmJQC4i60OVVXVxU00bh4nOV23JrJvVXVeVadVdfr48eNjbQbAg7WPNY7LJOsWsGcbfn7bcQD26M7hmE6N/W3NU95m9eGmR0kudjAOwBFs+4njuxXfmyf5uaquszgDanZrfFZVW49vud0A3NHHhuO9v/yntY3r5e9NV5L/sLTu8TrTQvU0fpJk+U1/23EADmzTlePzLG4j8iLJyRjjdZJfbm4pUlXn01lMybT2sHxV9834FJRkccuQnY0DcHibrhy/TPJm+vrQc84/NHaIcQAOy91xAWgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoOWLTU8YY8yS/CnJi6r6esX4WZKr6eG8qt4cchyAw1objjHGSZJ5Fm/c8xXjZ0muqurH6fF8jPF9Vb06xDgAh7c2HFV1keRiCsgqr6rq2dLzL8cYpwccB+DA7rzGMR3Ceu9TSJKrMcbzfY/fbasB2NbGNY41bg5h3Xa9NLbPcQCOYJuzqh6tGZsdYHylMcbZGOPtGOPtr7/+uuYlALiLB3c6blWdV9VpVZ0+fvz42JsD8ODsIxyzI48DsEfbhONtVh9OepTk4gDjABzBncNRVddZnOE0uzU0q6qf9z1+1+0GYDsfG44PLVS/TnJ282C63uPnA44DcGCbrhyfJ3mZ5EWSkzHG6yS/VNV5sliIns5iejn9yHz5qu59jwNweJuuHL9M8mb6+tBzzje8xl7HATisB3c6LgD7JRwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC3CAUCLcADQIhwAtAgHAC1fbPsCY4yXSb5J8l2S6yQvk1xX1fnSc86SXE0P51X15tZrbDUOwOFsHY7JSZJ3WYTjfPmN/eZNv6p+nB7PxxjfV9WrXYwDcFg7CUdVfbVm+FVVPVt67uUY43SH4wAc0F7XOMYYsyTzFUNXY4zn247vbEMB+Gg7+cQxvYnPsjhUdbJ0qGqef65NLLteGttmHIAD20U4LrNYDL9MkjHG1Rjjp6p6keTRmp+b7WD8PdOayFmSPHnyZM2PA3AXWx+qqqqLm2jcPE5yOsY42fa177g951V1WlWnjx8/PsYmADxo+1rjuEyybgF7tuHntx0HYE+2Csd0auxva57yNqsPNz1KcrGDcQAObBefOL5b8b15kp+r6jqLM6Bmt8ZnVbX1+LYbDkDfVuGY1jaul783XUn+w9K6x+tMi9XT+EmS5Tf9bccBOKCtz6qqqvPpTKZkWntYvqr7ZnwKSrK4ZcjOxgE4rF1dOX5+zHEADsfdcQFoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKDli2NvAO97+ue/HeX3/v2vfzzK7wXuF584AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAFuEAoEU4AGgRDgBahAOAlntzr6oxxlmSq+nhvKreHHN7AD5X9yIcUzRSVT9Oj+djjO+r6tVxtwzg83MvwpHkVVU9u3lQVZdjjOfH3CCAz9UnH44xxizJyYqh6zHG86r6+cCb9GAd63buiVu6w33yyYcjyTzJ9YrvX2URFOF4APwfJHB/3IdwPMo/F8WXXSf519vfnNZDzqaH/zfG+O/9bdq98mWSfxx7Iz414/XWL2Fed8+c7kdnXv9t3eB9CEdLVZ0nOT/2dnxqxhhvq+r02Nvx0JjX3TOn+7HLeb0v13E8WvG9WZL/PfB2AHz27kM43mYRidseJbk47KYA8MmHo6quk1xOZ1ctmzmjqsXhu/0wr7tnTvdjZ/M6qmpXr7U304L3V1X17fT4JItrO1wACHBg9yIcye/xuMzisJVbjgAcyb0JB5uNMeZZRNUhPO4N++398+BOx/3MnST5z2k96DqLEwu+rarfTyJws8jNpkOhf6mqr1eMrZ0/8/tha+bVfruFm3v5Jbm5LdO309rw8vhO91nheGCq6g9jjNnyjnPDzSLXm97YvsniNO/5ivG182d+V9s0r4n99q7GGGfTtWu/P07yX5kisrd9tqp8PZCvJC83jL9b8b1fjr3dn9pXFn8Br5qrtfNnfu88r/bbu83nLMnZiu//luT5x8zdXef2kz8dl93YdLPIA2/OvbNp/szvfpjXteZJvl9xqcJlkvk+91nheGCmHeLm6z+WhjbdLJL1Ns2f+d2C/bavFmtAz+r9w3vzLNaJ9rbPWuN4WC6Sxf9XkiRjjMsxxk9V9SLNm0Xynk3zZ37vzn57R7V0AkGSjDFeJrmsqovpU8Ne9lnheEBu/uEtP54Wuz7rv8z4tNlvd2M69PSXJP++79/lUNXDd5nk5o6Ybha5nU3zZ353x37b9zrJ17cOXe1lnxWOB2L6C+1DV3Nexc0it7Vp/szvHdhvd2NaF3p969Pb3vZZ4Xg4rpKsOvf6NMlFuVnkVjbNn/m9M/vtlqZrMX5cjsb032pfZ0/7rHA8ECvOrLjZoX5Y2qFeZ3EM9Gbcf7272qqP78nm+TO/6703r/bb7UwL4G+XTiyY3TqVdi/7rHtVPTDTR9brTB9Ba/XtBdwscoXpnkkvs7jK+SSL21C/q/evzP3g/Jnf933kvNpvm6Z5/eUDw3+4ifI+9lnhAKDFoSoAWoQDgBbhAKBFOABoEQ4AWoQDgBbhAKBFOABoEQ4AWv4fOJ1d2CWNSN8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad4498-9888-4707-a7d5-504ac1c4dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phat_from_regressor(model, test_data):\n",
    "    X_torch = torch.from_numpy(X).float()\n",
    "    X_torch= Tensor(X_torch)\n",
    "    model.eval()\n",
    "    phat = model(X_torch)\n",
    "    phat = phat.squeeze()\n",
    "    phat=phat.detach().numpy().flatten()#detaches it from the computational history/prevent future computations from being tracked\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
