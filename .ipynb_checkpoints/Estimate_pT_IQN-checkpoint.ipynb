{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc1f538-b5de-4211-8a45-b2dcffb52552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard scientific python module\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as op\n",
    "\n",
    "# standard symbolic algebra module\n",
    "#import sympy as sm\n",
    "#sm.init_printing()\n",
    "\n",
    "# module to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "# split data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# linearly transform a feature to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 18\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set usetex = False if LaTex is not \n",
    "# available on your system or if the \n",
    "# rendering is too slow\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "#seed = 128\n",
    "#rnd  = np.random.RandomState(seed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf71fd9-26e5-42f9-b81d-d4d5f469f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rawRecoDatapT</th>\n",
       "      <th>rawRecoDataeta</th>\n",
       "      <th>rawRecoDataphi</th>\n",
       "      <th>rawRecoDatam</th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0586</td>\n",
       "      <td>3.511970</td>\n",
       "      <td>1.503010</td>\n",
       "      <td>5.69919</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47.9465</td>\n",
       "      <td>0.776638</td>\n",
       "      <td>-1.251970</td>\n",
       "      <td>6.72517</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.5200</td>\n",
       "      <td>-1.084730</td>\n",
       "      <td>1.834230</td>\n",
       "      <td>4.06446</td>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23.2719</td>\n",
       "      <td>-2.822960</td>\n",
       "      <td>0.216718</td>\n",
       "      <td>3.50878</td>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.4644</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>1.306930</td>\n",
       "      <td>4.11101</td>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rawRecoDatapT  rawRecoDataeta  rawRecoDataphi  rawRecoDatam  \\\n",
       "0           0        29.0586        3.511970        1.503010       5.69919   \n",
       "1           1        47.9465        0.776638       -1.251970       6.72517   \n",
       "2           2        29.5200       -1.084730        1.834230       4.06446   \n",
       "3           3        23.2719       -2.822960        0.216718       3.50878   \n",
       "4           4        30.4644        2.985000        1.306930       4.11101   \n",
       "\n",
       "   RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  genDataeta  \\\n",
       "0     40.3892      3.41479      1.47023   12.53740    45.4608    3.379820   \n",
       "1     40.3892      3.41479      1.47023   12.53740    56.2643    0.811412   \n",
       "2     29.3586     -1.17862      1.84039    9.95503    34.6377   -1.131020   \n",
       "3     20.9593      2.13374     -2.86886    9.55921    27.4120   -2.842550   \n",
       "4     35.2909      2.96499      1.36464   10.69580    30.3263    3.040300   \n",
       "\n",
       "   genDataphi  genDatam       tau  \n",
       "0    1.470130  13.24440  0.536525  \n",
       "1   -1.324120  10.55060  0.130536  \n",
       "2    1.801820   7.65844  0.500162  \n",
       "3    0.345529   5.18675  0.490624  \n",
       "4    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2e6ff8-1552-4ff9-aa0e-0cb31a6dacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawRecoDatam</th>\n",
       "      <th>RecoDatapT</th>\n",
       "      <th>RecoDataeta</th>\n",
       "      <th>RecoDataphi</th>\n",
       "      <th>RecoDatam</th>\n",
       "      <th>genDatapT</th>\n",
       "      <th>genDataeta</th>\n",
       "      <th>genDataphi</th>\n",
       "      <th>genDatam</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.69919</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>45.4608</td>\n",
       "      <td>3.379820</td>\n",
       "      <td>1.470130</td>\n",
       "      <td>13.24440</td>\n",
       "      <td>0.536525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.72517</td>\n",
       "      <td>40.3892</td>\n",
       "      <td>3.41479</td>\n",
       "      <td>1.47023</td>\n",
       "      <td>12.53740</td>\n",
       "      <td>56.2643</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>-1.324120</td>\n",
       "      <td>10.55060</td>\n",
       "      <td>0.130536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.06446</td>\n",
       "      <td>29.3586</td>\n",
       "      <td>-1.17862</td>\n",
       "      <td>1.84039</td>\n",
       "      <td>9.95503</td>\n",
       "      <td>34.6377</td>\n",
       "      <td>-1.131020</td>\n",
       "      <td>1.801820</td>\n",
       "      <td>7.65844</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.50878</td>\n",
       "      <td>20.9593</td>\n",
       "      <td>2.13374</td>\n",
       "      <td>-2.86886</td>\n",
       "      <td>9.55921</td>\n",
       "      <td>27.4120</td>\n",
       "      <td>-2.842550</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>5.18675</td>\n",
       "      <td>0.490624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.11101</td>\n",
       "      <td>35.2909</td>\n",
       "      <td>2.96499</td>\n",
       "      <td>1.36464</td>\n",
       "      <td>10.69580</td>\n",
       "      <td>30.3263</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>1.341270</td>\n",
       "      <td>5.74890</td>\n",
       "      <td>0.417064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rawRecoDatam  RecoDatapT  RecoDataeta  RecoDataphi  RecoDatam  genDatapT  \\\n",
       "0       5.69919     40.3892      3.41479      1.47023   12.53740    45.4608   \n",
       "1       6.72517     40.3892      3.41479      1.47023   12.53740    56.2643   \n",
       "2       4.06446     29.3586     -1.17862      1.84039    9.95503    34.6377   \n",
       "3       3.50878     20.9593      2.13374     -2.86886    9.55921    27.4120   \n",
       "4       4.11101     35.2909      2.96499      1.36464   10.69580    30.3263   \n",
       "\n",
       "   genDataeta  genDataphi  genDatam       tau  \n",
       "0    3.379820    1.470130  13.24440  0.536525  \n",
       "1    0.811412   -1.324120  10.55060  0.130536  \n",
       "2   -1.131020    1.801820   7.65844  0.500162  \n",
       "3   -2.842550    0.345529   5.18675  0.490624  \n",
       "4    3.040300    1.341270   5.74890  0.417064  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:,4:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28b26ef-0109-4c4b-a53c-d62d24f8dc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.69919   ,  3.41479   ,  1.47023   , ...,  1.47013   ,\n",
       "        13.2444    ,  0.53652539],\n",
       "       [ 6.72517   ,  3.41479   ,  1.47023   , ..., -1.32412   ,\n",
       "        10.5506    ,  0.13053623],\n",
       "       [ 4.06446   , -1.17862   ,  1.84039   , ...,  1.80182   ,\n",
       "         7.65844   ,  0.50016227],\n",
       "       ...,\n",
       "       [ 3.89597   , -1.49005   , -1.42238   , ...,  1.85921   ,\n",
       "         8.8265    ,  0.11958587],\n",
       "       [ 1.62073   , -0.654844  , -1.26413   , ..., -1.21499   ,\n",
       "         4.87261   ,  0.5541255 ],\n",
       "       [ 5.24735   , -1.10686   ,  1.3181    , ...,  1.33004   ,\n",
       "         3.94825   ,  0.05202467]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = ['genData', 'RecoData']\n",
    "kinematics=['pT','eta','phi','m']\n",
    "targets = kinematics#for reco level, but same names\n",
    "Networks = ['RecoNN', 'genNN']\n",
    "\n",
    "target = df['RecoDatapT'].to_numpy()\n",
    "data =  df.drop('RecoDatapT', axis=1).to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e16158f-8207-4190-bb15-59784d3a5e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.3892, 40.3892, 29.3586, ..., 27.0034, 23.8815, 33.2208])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610a3400-3651-4125-8256-e5ab5f48b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = 1\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, target, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ff906a-7669-482b-9588-0e101d0a1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target shape (80000,)\n",
      "input data shape (100000, 9)\n"
     ]
    }
   ],
   "source": [
    "# train_targets = train_targets.reshape(-1,1)\n",
    "# test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "print('target shape', train_targets.shape)\n",
    "print('input data shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dafd1bfd-b236-45a2-aa29-810cb7966bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape =  (80000, 9) \n",
      "\n",
      "test_data shape =  (20000, 9) \n",
      "\n",
      "train_targets shape =  (80000, 1) \n",
      "\n",
      "test_targets shape =  (20000, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "\n",
    "sets= [train_data, test_data, train_targets, test_targets]\n",
    "set_names = ['train_data', 'test_data', 'train_targets', 'test_targets']\n",
    "# vnames = [name for name in globals() if globals()[name] is variable]\n",
    "\n",
    "def variable_string(variable):\n",
    "    return [k for k, v in locals().items() if v == variable][0]\n",
    "\n",
    "for var_name, var in zip(set_names, sets):\n",
    "    print(var_name \n",
    "          + ' shape = ', var.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68142b60-eb97-4866-94f6-3d90a25782ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()#this is always recommended for logistic regression\n",
    "# train_data= sc.fit_transform(train_data)\n",
    "# test_data = sc.transform(test_data)\n",
    "# train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3547fd-700a-4b2d-b1c9-7cf8c0297b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([ 1.5000,  0.7498, -1.5463,  5.1183, 31.7755,  0.7550, -1.5539,  5.7298,\n",
      "         0.8427]), 'y': tensor([31.2903])} <__main__.CustomDataset object at 0x7f8944ff59d0>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "print(train_dataset[0], train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b8c07a-a637-4f2b-8bc9-0c0184174e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=6, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6071393-9cc1-4735-8c2a-4e798d630b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e96587-d19a-4e3d-ae85-65f73f17a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape =  (80000, 9)\n"
     ]
    }
   ],
   "source": [
    "# n_examples, n_inputs = train_data.shape\n",
    "# n_outputs, n_hidden = 1, 16\n",
    "print('train_data.shape = ',train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876d298d-2338-4843-beda-352a5d045eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (13): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (17): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=5, \n",
    "               hidden_size=16, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a5d5858-3b3e-4788-85eb-8146b6837564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %writefile training/RegressionEngine.py\n",
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def quadratic_loss(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "    @staticmethod\n",
    "    def average_quadratic_loss(targets, outputs):\n",
    "    # f and t must be of the same shape\n",
    "        return  torch.mean((outputs - targets)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_absolute_error(targets, outputs):\n",
    "    # f and t must be of the same shape\n",
    "        return  torch.mean(abs(outputs - targets))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def average_cross_entropy_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        loss = torch.where(targets > 0.5, torch.log(outputs), torch.log(1 - outputs))\n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_quantile_loss(targets, outputs):\n",
    "        # f and t must be of the same shape\n",
    "        tau = torch.rand(outputs.shape)\n",
    "        return torch.mean(torch.where(targets >= outputs, \n",
    "                                      tau * abs(targets - outputs), \n",
    "                                      (1 - tau)*abs(outputs - targets)))\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()#only optimize weights for the current batch, otherwise it's meaningless!\n",
    "            inputs = data[\"x\"]\n",
    "            targets = data[\"y\"]\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.average_quantile_loss(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.average_quantile_loss(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8240b6-104c-4260-8da2-0a3c4393b79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (697863518.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_48266/697863518.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    ir save_model:\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs, save_model=False):\n",
    "    train_losses, test_losses = [], []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    # EPOCHS=22\n",
    "    EPOCHS=epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.evaluate(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \"goodmodel.pth\")\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break\n",
    "    \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    train_losses=np.array(train_losses); test_losses=np.array(test_losses)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    epoch_list = np.arange(1, train_losses.shape[0]+1)\n",
    "    ax.plot(epoch_list, train_losses, label = 'Train')\n",
    "    ax.plot(epoch_list, test_losses, label='Test')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    return train_losses, test_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97be35-ab2c-4f0f-b82f-c32df5050768",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_losses, test_losses=train(optimizer, \n",
    "      engine =RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 10,\n",
    "      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "72f552c7-56b9-4059-8608-3459d65c5144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11,), (11,))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_losses).shape, np.array(test_losses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "01c3cac7-20aa-46b8-ace9-031b77cb41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(1, train_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "fce7cc65-2266-4098-b5fc-8950a02b4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]#y is Z values. I could add here my computed p-value for each theta,\n",
    "            #and make a dataframe col1:theta, col2: Z, col3, phat, col4: computedp-value\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])# number of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS.flatten(), LABELS.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466955c7-7ca7-4272-8789-9c1a53b2f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [0.7690218  0.6086684  0.44430038 ... 0.7240402  0.49077702 0.6382137 ]\n",
      "\n",
      "actual labels (targets Z):  [31.1167 29.7691 30.5138 ... 35.866  34.0439 47.1551]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01ec1c3d-6b82-4f0f-b5cc-b464d00c4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.8277e+04, 1.4950e+03, 1.8000e+02, 3.3000e+01, 6.0000e+00,\n",
       "        3.0000e+00, 3.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]),\n",
       " array([ 20.0008  ,  37.75662 ,  55.51244 ,  73.268265,  91.02408 ,\n",
       "        108.7799  , 126.53572 , 144.29155 , 162.04736 , 179.80319 ,\n",
       "        197.559   ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad4498-9888-4707-a7d5-504ac1c4dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phat_from_regressor(model, test_data):\n",
    "    X_torch = torch.from_numpy(X).float()\n",
    "    X_torch= Tensor(X_torch)\n",
    "    model.eval()\n",
    "    phat = model(X_torch)\n",
    "    phat = phat.squeeze()\n",
    "    phat=phat.detach().numpy().flatten()#detaches it from the computational history/prevent future computations from being tracked\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
